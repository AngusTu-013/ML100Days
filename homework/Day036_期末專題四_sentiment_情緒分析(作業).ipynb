{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 題目:電商產品評分文件以機器學習方式分辨是否為正向或負向\n",
    "\n",
    "#說明：輸入文件 positive.review 和 negative.review，兩者都是XML檔。我們用BeautifulSoup讀進來，\n",
    "#擷取review_text，然後用NLTK自建Tokenizer。 先產生 word-to-index map 再產生 word-frequency vectors。\n",
    "#之後 shuffle data 創造 train/test splits，留100個給 test 用。接著用Logistic Regression 分類器\n",
    "#找出訓練組和測試組的準確度(Accuracy)。接著我們可以看看每個單字的正負權重，可以訂一個閥值，\n",
    "#比方絕對值大於正負0.5，以確認情緒是顯著的。最後我們找出根據現有演算法歸類錯誤最嚴重的正向情緒和負向\n",
    "#情緒的例子。\n",
    "\n",
    "#延伸:可用不同的tokenizer，不同的tokens_to_vector，不同的ML分類器做改進準確率的比較。最後可用您的\n",
    "#model去預測unlabeled.review檔的內容。\n",
    "\n",
    "#範例程式檔名: sentiment_情緒分析.py，以LogisticRegression 方式完成情緒分析。\n",
    "#模組: sklearn, bs4, numpy, nltk\n",
    "#輸入檔：stopwords.txt, /electronics 下 positive.review, negative.review\n",
    "#成績：辨識百分率\n",
    "\n",
    "#注意事項：nltk 需要有 punkt corpus 和 wordnet  資源\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet') \n",
    "#資料檔需在適當位置 jupyter 或 colab 才能看到，用colab時要上傳 data 到 ./sample_data 或 mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords(作業數據).txt'))\n",
    "\n",
    "# 另一個 stopwords 的來源\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀正向與負向 reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review', encoding='utf-8').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review', encoding='utf-8').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 11092\n"
     ]
    }
   ],
   "source": [
    "# 基於nltk自建 tokenizer\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # 將字串改為tokens\n",
    "    tokens = [t for t in tokens if len(t) > 2] # 去除短字\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # 去除大小寫\n",
    "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n",
    "    return tokens\n",
    "\n",
    "# 先產生 word-to-index map 再產生 word-frequency vectors\n",
    "# 同時儲存 tokenized 版本未來不需再做 tokenization\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # 正規化數據提升未來準確度\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data 創造 train/test splits\n",
    "# 多次嘗試!\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7789473684210526\n",
      "Test accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "# 最後 100 列是測試用\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.6706167333886154\n",
      "bad -0.7784231654619602\n",
      "cable 0.6756359370501488\n",
      "time -0.7193940298142807\n",
      "'ve 0.8055282001161009\n",
      "month -0.869754559817761\n",
      "sound 1.0158773627921514\n",
      "lot 0.7457494050426247\n",
      "you 1.0719763322973621\n",
      "n't -2.1325371473549652\n",
      "easy 1.7089834444088294\n",
      "quality 1.5146575007502503\n",
      "company -0.5199100096859878\n",
      "item -0.9148189670362915\n",
      "wa -1.602551580542679\n",
      "perfect 0.9547045514463003\n",
      "fast 0.8860622233632048\n",
      "ha 0.6952331140302515\n",
      "price 2.6864656441120713\n",
      "value 0.5422277898149388\n",
      "money -1.067381340843681\n",
      "memory 1.0563239724999358\n",
      "buy -0.8710023580781587\n",
      "bit 0.5816672767246565\n",
      "happy 0.6667124614510636\n",
      "pretty 0.7475084076774344\n",
      "doe -1.1096329525053\n",
      "highly 1.0057215279292153\n",
      "recommend 0.6450364683168788\n",
      "customer -0.678210912891818\n",
      "support -0.9378196629569586\n",
      "little 0.9753802199297265\n",
      "returned -0.7708614225198367\n",
      "excellent 1.3539794078012646\n",
      "love 1.1984117125204419\n",
      "home 0.5078152740151647\n",
      "week -0.7624435030881691\n",
      "using 0.6321722677473885\n",
      "laptop 0.5197028486470675\n",
      "video 0.572604519380853\n",
      "poor -0.7888069415991408\n",
      "then -0.9997715258414486\n",
      "tried -0.8639493980152683\n",
      "static -0.5058690290027466\n",
      "try -0.6283993552848609\n",
      "space 0.6221063174860728\n",
      "comfortable 0.6511498328484328\n",
      "hour -0.5173021970311394\n",
      "expected 0.58167830772418\n",
      "speaker 0.8148951811319011\n",
      "warranty -0.6085982489161277\n",
      "stopped -0.5294055991420528\n",
      "paper 0.6390743475316293\n",
      "return -1.1989578427373904\n",
      "waste -0.9951198030753451\n",
      "refund -0.6066321122896344\n"
     ]
    }
   ],
   "source": [
    "# 列出每個字的正負 weight\n",
    "# 用不同的 threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出歸類錯誤的例子\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most wrong positive review (prob = 0.3481276372173868, pred = 0.0):\n",
      "\n",
      "I didn't buy this on Amazon but wanted to say this device is great. The only bad thing was MY laptop is old!  Can't go wrong with this one\n",
      "\n",
      "Most wrong negative review (prob = 0.5994882275478557, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 只列出最糟的\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
