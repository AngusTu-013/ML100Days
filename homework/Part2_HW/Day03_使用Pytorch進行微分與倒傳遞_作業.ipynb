{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxtArZ7u16sr"
   },
   "source": [
    "### 作業目標: 使用Pytorch進行微分與倒傳遞\n",
    "這份作業我們會實作微分與倒傳遞以及使用Pytorch的Autograd。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_BwC7sg16ss"
   },
   "source": [
    "### 使用Pytorch實作微分與倒傳遞\n",
    "\n",
    "這裡我們很簡單的實作兩層的神經網路進行回歸問題，其中loss function為L2 loss\n",
    "\n",
    "$$\n",
    "L2\\_loss = (y_{pred}-y)^2\n",
    "$$\n",
    "\n",
    "兩層經網路如下所示\n",
    "$$\n",
    "y_{pred} = ReLU(XW_1)W_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ocsA8ch-16st"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o2v8hkG616sz",
    "outputId": "0b737d18-59c2-4bb7-f541-e0ca6ab51a11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33839764.0\n",
      "1 31205788.0\n",
      "2 32969728.0\n",
      "3 32215580.0\n",
      "4 26178854.0\n",
      "5 16646683.0\n",
      "6 8876494.0\n",
      "7 4398068.5\n",
      "8 2341312.5\n",
      "9 1427207.0\n",
      "10 998654.625\n",
      "11 767756.0\n",
      "12 623547.3125\n",
      "13 521939.4375\n",
      "14 444397.53125\n",
      "15 382444.21875\n",
      "16 331626.75\n",
      "17 289332.25\n",
      "18 253696.859375\n",
      "19 223444.40625\n",
      "20 197606.109375\n",
      "21 175448.140625\n",
      "22 156296.625\n",
      "23 139659.796875\n",
      "24 125182.4921875\n",
      "25 112549.28125\n",
      "26 101453.609375\n",
      "27 91684.3046875\n",
      "28 83031.5859375\n",
      "29 75343.265625\n",
      "30 68494.9140625\n",
      "31 62377.23828125\n",
      "32 56898.7734375\n",
      "33 51980.0703125\n",
      "34 47553.91796875\n",
      "35 43562.22265625\n",
      "36 39958.1796875\n",
      "37 36698.26953125\n",
      "38 33742.25\n",
      "39 31062.29296875\n",
      "40 28626.740234375\n",
      "41 26406.251953125\n",
      "42 24381.021484375\n",
      "43 22529.369140625\n",
      "44 20834.982421875\n",
      "45 19282.8125\n",
      "46 17859.27734375\n",
      "47 16551.751953125\n",
      "48 15349.9921875\n",
      "49 14245.232421875\n",
      "50 13228.9453125\n",
      "51 12291.9814453125\n",
      "52 11427.984375\n",
      "53 10630.908203125\n",
      "54 9893.6962890625\n",
      "55 9212.60546875\n",
      "56 8582.1923828125\n",
      "57 7999.1552734375\n",
      "58 7459.46630859375\n",
      "59 6959.11572265625\n",
      "60 6495.1044921875\n",
      "61 6064.43994140625\n",
      "62 5664.423828125\n",
      "63 5292.69287109375\n",
      "64 4947.419921875\n",
      "65 4626.28955078125\n",
      "66 4327.38330078125\n",
      "67 4049.119384765625\n",
      "68 3789.921630859375\n",
      "69 3548.483642578125\n",
      "70 3323.298828125\n",
      "71 3113.34423828125\n",
      "72 2917.382080078125\n",
      "73 2734.525146484375\n",
      "74 2563.83935546875\n",
      "75 2404.467529296875\n",
      "76 2255.55712890625\n",
      "77 2116.33984375\n",
      "78 1986.23046875\n",
      "79 1864.50537109375\n",
      "80 1750.62744140625\n",
      "81 1644.095703125\n",
      "82 1544.3487548828125\n",
      "83 1450.951904296875\n",
      "84 1363.5235595703125\n",
      "85 1281.5904541015625\n",
      "86 1204.8292236328125\n",
      "87 1132.892822265625\n",
      "88 1065.4388427734375\n",
      "89 1002.1920776367188\n",
      "90 942.8764038085938\n",
      "91 887.2188110351562\n",
      "92 834.9844360351562\n",
      "93 785.9798583984375\n",
      "94 739.9813232421875\n",
      "95 696.7897338867188\n",
      "96 656.2175903320312\n",
      "97 618.1263427734375\n",
      "98 582.3489990234375\n",
      "99 548.6939697265625\n",
      "100 517.070556640625\n",
      "101 487.34991455078125\n",
      "102 459.3968200683594\n",
      "103 433.10821533203125\n",
      "104 408.3831481933594\n",
      "105 385.1230773925781\n",
      "106 363.2442626953125\n",
      "107 342.662109375\n",
      "108 323.28094482421875\n",
      "109 305.03472900390625\n",
      "110 287.8592529296875\n",
      "111 271.682373046875\n",
      "112 256.45745849609375\n",
      "113 242.11083984375\n",
      "114 228.59214782714844\n",
      "115 215.85218811035156\n",
      "116 203.856201171875\n",
      "117 192.54644775390625\n",
      "118 181.8898162841797\n",
      "119 171.8418426513672\n",
      "120 162.3660430908203\n",
      "121 153.4263153076172\n",
      "122 145.00094604492188\n",
      "123 137.05477905273438\n",
      "124 129.55873107910156\n",
      "125 122.48228454589844\n",
      "126 115.80540466308594\n",
      "127 109.50654602050781\n",
      "128 103.55825805664062\n",
      "129 97.9468994140625\n",
      "130 92.64676666259766\n",
      "131 87.64459991455078\n",
      "132 82.91934204101562\n",
      "133 78.4570541381836\n",
      "134 74.2423095703125\n",
      "135 70.26007080078125\n",
      "136 66.50057983398438\n",
      "137 62.945987701416016\n",
      "138 59.5884895324707\n",
      "139 56.415626525878906\n",
      "140 53.416709899902344\n",
      "141 50.58309555053711\n",
      "142 47.90604019165039\n",
      "143 45.37277603149414\n",
      "144 42.97785568237305\n",
      "145 40.71393966674805\n",
      "146 38.571598052978516\n",
      "147 36.545597076416016\n",
      "148 34.62930679321289\n",
      "149 32.8170166015625\n",
      "150 31.101512908935547\n",
      "151 29.478343963623047\n",
      "152 27.94243812561035\n",
      "153 26.489826202392578\n",
      "154 25.113515853881836\n",
      "155 23.810741424560547\n",
      "156 22.57940673828125\n",
      "157 21.415687561035156\n",
      "158 20.313337326049805\n",
      "159 19.269254684448242\n",
      "160 18.28107452392578\n",
      "161 17.344736099243164\n",
      "162 16.457508087158203\n",
      "163 15.61700439453125\n",
      "164 14.820435523986816\n",
      "165 14.065740585327148\n",
      "166 13.35092544555664\n",
      "167 12.672914505004883\n",
      "168 12.030472755432129\n",
      "169 11.421621322631836\n",
      "170 10.843941688537598\n",
      "171 10.296576499938965\n",
      "172 9.77808952331543\n",
      "173 9.285782814025879\n",
      "174 8.818697929382324\n",
      "175 8.37598705291748\n",
      "176 7.955986499786377\n",
      "177 7.557712078094482\n",
      "178 7.179586887359619\n",
      "179 6.821286201477051\n",
      "180 6.480814456939697\n",
      "181 6.158200263977051\n",
      "182 5.851582050323486\n",
      "183 5.560798168182373\n",
      "184 5.285030364990234\n",
      "185 5.022852420806885\n",
      "186 4.774282932281494\n",
      "187 4.538448810577393\n",
      "188 4.314441680908203\n",
      "189 4.101346969604492\n",
      "190 3.8994600772857666\n",
      "191 3.7074429988861084\n",
      "192 3.5253868103027344\n",
      "193 3.352292537689209\n",
      "194 3.18778395652771\n",
      "195 3.0314817428588867\n",
      "196 2.8831632137298584\n",
      "197 2.7422986030578613\n",
      "198 2.6085128784179688\n",
      "199 2.481208086013794\n",
      "200 2.360302448272705\n",
      "201 2.2453205585479736\n",
      "202 2.136262893676758\n",
      "203 2.0325238704681396\n",
      "204 1.9339251518249512\n",
      "205 1.8401778936386108\n",
      "206 1.75101637840271\n",
      "207 1.6664239168167114\n",
      "208 1.5859112739562988\n",
      "209 1.509377121925354\n",
      "210 1.4365733861923218\n",
      "211 1.367266058921814\n",
      "212 1.3015162944793701\n",
      "213 1.238962173461914\n",
      "214 1.179438591003418\n",
      "215 1.1228505373001099\n",
      "216 1.069002389907837\n",
      "217 1.0178236961364746\n",
      "218 0.9690872430801392\n",
      "219 0.9227166771888733\n",
      "220 0.8786087036132812\n",
      "221 0.8366761207580566\n",
      "222 0.7967864274978638\n",
      "223 0.7589070796966553\n",
      "224 0.7227367162704468\n",
      "225 0.6882708072662354\n",
      "226 0.6556259989738464\n",
      "227 0.6245136260986328\n",
      "228 0.5948141813278198\n",
      "229 0.5667144656181335\n",
      "230 0.5398397445678711\n",
      "231 0.514305055141449\n",
      "232 0.4899310767650604\n",
      "233 0.46672534942626953\n",
      "234 0.44475042819976807\n",
      "235 0.42377713322639465\n",
      "236 0.4038086533546448\n",
      "237 0.3848172128200531\n",
      "238 0.36666712164878845\n",
      "239 0.3494666516780853\n",
      "240 0.3329835832118988\n",
      "241 0.31735658645629883\n",
      "242 0.3024788796901703\n",
      "243 0.288240522146225\n",
      "244 0.2747301161289215\n",
      "245 0.2618910074234009\n",
      "246 0.24962429702281952\n",
      "247 0.23796965181827545\n",
      "248 0.22683003544807434\n",
      "249 0.21624238789081573\n",
      "250 0.2061680555343628\n",
      "251 0.19653555750846863\n",
      "252 0.18734435737133026\n",
      "253 0.17863446474075317\n",
      "254 0.1703554093837738\n",
      "255 0.16239002346992493\n",
      "256 0.15483523905277252\n",
      "257 0.1476689875125885\n",
      "258 0.14080344140529633\n",
      "259 0.1342562586069107\n",
      "260 0.12806685268878937\n",
      "261 0.12210676074028015\n",
      "262 0.11645921319723129\n",
      "263 0.11107336729764938\n",
      "264 0.10592368245124817\n",
      "265 0.10101211071014404\n",
      "266 0.09638064354658127\n",
      "267 0.09192986786365509\n",
      "268 0.08768458664417267\n",
      "269 0.08362501114606857\n",
      "270 0.07979501783847809\n",
      "271 0.07610667496919632\n",
      "272 0.07259252667427063\n",
      "273 0.06925283372402191\n",
      "274 0.06608257442712784\n",
      "275 0.06307381391525269\n",
      "276 0.06017693132162094\n",
      "277 0.05739789456129074\n",
      "278 0.05478096753358841\n",
      "279 0.05226514860987663\n",
      "280 0.049879178404808044\n",
      "281 0.04758448153734207\n",
      "282 0.04542311653494835\n",
      "283 0.043352093547582626\n",
      "284 0.04135815054178238\n",
      "285 0.039474859833717346\n",
      "286 0.037685178220272064\n",
      "287 0.03597928583621979\n",
      "288 0.034332819283008575\n",
      "289 0.03277294337749481\n",
      "290 0.0312831737101078\n",
      "291 0.02986535057425499\n",
      "292 0.028497641906142235\n",
      "293 0.027220023795962334\n",
      "294 0.025976747274398804\n",
      "295 0.024799086153507233\n",
      "296 0.02367149107158184\n",
      "297 0.02261422947049141\n",
      "298 0.021595723927021027\n",
      "299 0.020628876984119415\n",
      "300 0.019707603380084038\n",
      "301 0.018811313435435295\n",
      "302 0.017968781292438507\n",
      "303 0.01715722307562828\n",
      "304 0.016383957117795944\n",
      "305 0.01566029153764248\n",
      "306 0.014961080625653267\n",
      "307 0.014304564334452152\n",
      "308 0.013660755008459091\n",
      "309 0.013056396506726742\n",
      "310 0.012479536235332489\n",
      "311 0.011925038881599903\n",
      "312 0.011392409913241863\n",
      "313 0.010895443148911\n",
      "314 0.010417712852358818\n",
      "315 0.009954884648323059\n",
      "316 0.009515918791294098\n",
      "317 0.009102867916226387\n",
      "318 0.008701437152922153\n",
      "319 0.008321771398186684\n",
      "320 0.007959369570016861\n",
      "321 0.007613962050527334\n",
      "322 0.007286010775715113\n",
      "323 0.0069656833074986935\n",
      "324 0.0066689131781458855\n",
      "325 0.0063884062692523\n",
      "326 0.00611383281648159\n",
      "327 0.005855068564414978\n",
      "328 0.005607898812741041\n",
      "329 0.005372644402086735\n",
      "330 0.005142217501997948\n",
      "331 0.004925376269966364\n",
      "332 0.004718559328466654\n",
      "333 0.004519347101449966\n",
      "334 0.00433183740824461\n",
      "335 0.004153958521783352\n",
      "336 0.00397899467498064\n",
      "337 0.003814300522208214\n",
      "338 0.003655806416645646\n",
      "339 0.0035105731803923845\n",
      "340 0.0033698943443596363\n",
      "341 0.003230402246117592\n",
      "342 0.0031009879894554615\n",
      "343 0.002973875030875206\n",
      "344 0.0028558848425745964\n",
      "345 0.0027443566359579563\n",
      "346 0.002640312071889639\n",
      "347 0.002534117316827178\n",
      "348 0.002437073038890958\n",
      "349 0.002341942861676216\n",
      "350 0.002248485805466771\n",
      "351 0.002162883523851633\n",
      "352 0.0020794065203517675\n",
      "353 0.0019989453721791506\n",
      "354 0.00192364864051342\n",
      "355 0.001851413631811738\n",
      "356 0.0017828948330134153\n",
      "357 0.001716551254503429\n",
      "358 0.0016525491373613477\n",
      "359 0.001593477209098637\n",
      "360 0.001534821349196136\n",
      "361 0.0014805214013904333\n",
      "362 0.001426537986844778\n",
      "363 0.0013754600659012794\n",
      "364 0.001325304270721972\n",
      "365 0.001280286000110209\n",
      "366 0.0012356513179838657\n",
      "367 0.0011935102520510554\n",
      "368 0.0011512088822200894\n",
      "369 0.0011123698204755783\n",
      "370 0.001074951491318643\n",
      "371 0.0010380314197391272\n",
      "372 0.0010036637540906668\n",
      "373 0.000969930668361485\n",
      "374 0.0009385826997458935\n",
      "375 0.0009072490502148867\n",
      "376 0.0008766753599047661\n",
      "377 0.0008489083265885711\n",
      "378 0.0008211039239540696\n",
      "379 0.0007953371969051659\n",
      "380 0.0007700607529841363\n",
      "381 0.0007460318738594651\n",
      "382 0.0007240143604576588\n",
      "383 0.0007011546986177564\n",
      "384 0.0006799855618737638\n",
      "385 0.0006586983217857778\n",
      "386 0.0006377204554155469\n",
      "387 0.0006183321820572019\n",
      "388 0.0005995617830194533\n",
      "389 0.0005806207191199064\n",
      "390 0.0005644260672852397\n",
      "391 0.0005472475895658135\n",
      "392 0.0005302745848894119\n",
      "393 0.0005149563658051193\n",
      "394 0.0005007559084333479\n",
      "395 0.0004865906958002597\n",
      "396 0.0004730242653749883\n",
      "397 0.00045964104356244206\n",
      "398 0.0004468562838155776\n",
      "399 0.0004348701040726155\n",
      "400 0.0004220817645546049\n",
      "401 0.0004101227968931198\n",
      "402 0.00039935437962412834\n",
      "403 0.00038834920269437134\n",
      "404 0.0003785950248129666\n",
      "405 0.0003684124385472387\n",
      "406 0.00035784178180620074\n",
      "407 0.0003484561457298696\n",
      "408 0.0003395388484932482\n",
      "409 0.0003315365465823561\n",
      "410 0.0003225139225833118\n",
      "411 0.00031477882293984294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 0.00030594898271374404\n",
      "413 0.0002993274829350412\n",
      "414 0.00029172596987336874\n",
      "415 0.0002843031252268702\n",
      "416 0.0002776019391603768\n",
      "417 0.00027025595773011446\n",
      "418 0.00026396819157525897\n",
      "419 0.000257790059549734\n",
      "420 0.0002522027643863112\n",
      "421 0.0002457919472362846\n",
      "422 0.00023980948026292026\n",
      "423 0.00023410332505591214\n",
      "424 0.00022871461987961084\n",
      "425 0.0002244000497739762\n",
      "426 0.00021903218294028193\n",
      "427 0.0002139952703146264\n",
      "428 0.00020883878460153937\n",
      "429 0.00020531739573925734\n",
      "430 0.00020031564054079354\n",
      "431 0.0001961431698873639\n",
      "432 0.0001919066853588447\n",
      "433 0.0001881696516647935\n",
      "434 0.00018412043573334813\n",
      "435 0.00018020826973952353\n",
      "436 0.0001760190207278356\n",
      "437 0.0001726032205624506\n",
      "438 0.0001692256482783705\n",
      "439 0.00016537094779778272\n",
      "440 0.00016193214105442166\n",
      "441 0.0001584652200108394\n",
      "442 0.00015534287376794964\n",
      "443 0.00015238785999827087\n",
      "444 0.0001494140160502866\n",
      "445 0.00014647174975834787\n",
      "446 0.00014364758681040257\n",
      "447 0.00014093401841819286\n",
      "448 0.00013809726806357503\n",
      "449 0.00013600177771877497\n",
      "450 0.00013324266183190048\n",
      "451 0.00013074878370389342\n",
      "452 0.00012836190580856055\n",
      "453 0.00012580957263708115\n",
      "454 0.00012336783402133733\n",
      "455 0.00012123896885896102\n",
      "456 0.0001188223686767742\n",
      "457 0.00011655406706267968\n",
      "458 0.00011444953997852281\n",
      "459 0.00011227306094951928\n",
      "460 0.00011052883201045915\n",
      "461 0.00010879970795940608\n",
      "462 0.00010712018411140889\n",
      "463 0.00010531997395446524\n",
      "464 0.00010325020411983132\n",
      "465 0.00010132408351637423\n",
      "466 9.967011283151805e-05\n",
      "467 9.77855670498684e-05\n",
      "468 9.621568460715935e-05\n",
      "469 9.461973968427628e-05\n",
      "470 9.307653817813843e-05\n",
      "471 9.15641212486662e-05\n",
      "472 9.036779374582693e-05\n",
      "473 8.894877100829035e-05\n",
      "474 8.743808575673029e-05\n",
      "475 8.599711145507172e-05\n",
      "476 8.463456470053643e-05\n",
      "477 8.317725587403402e-05\n",
      "478 8.201422315323725e-05\n",
      "479 8.063478162512183e-05\n",
      "480 7.951350562507287e-05\n",
      "481 7.808327063685283e-05\n",
      "482 7.686381286475807e-05\n",
      "483 7.5864780228585e-05\n",
      "484 7.489735435228795e-05\n",
      "485 7.350068335654214e-05\n",
      "486 7.241268031066284e-05\n",
      "487 7.121130329323933e-05\n",
      "488 7.023220678092912e-05\n",
      "489 6.915585254319012e-05\n",
      "490 6.820115959271789e-05\n",
      "491 6.755728099960834e-05\n",
      "492 6.62181555526331e-05\n",
      "493 6.538611341966316e-05\n",
      "494 6.426338222809136e-05\n",
      "495 6.336146907415241e-05\n",
      "496 6.232781743165106e-05\n",
      "497 6.164503429317847e-05\n",
      "498 6.072908581700176e-05\n",
      "499 5.975182284601033e-05\n"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "###<your code>###\n",
    "x = torch.randn((N, D_in)).to(device)\n",
    "y = torch.randn((N, D_out)).to(device)\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "###<your code>###\n",
    "W1 = torch.randn((D_in, H), requires_grad=True).to(device)\n",
    "W2 = torch.randn((H, D_out), requires_grad=True).to(device)\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(500):\n",
    "    # 向前傳遞: 計算y_pred\n",
    "    ###<your code>###\n",
    "    # 𝑦_𝑝𝑟𝑒𝑑 =𝑅𝑒𝐿𝑈(𝑋𝑊1)𝑊2\n",
    "    h = torch.matmul(x, W1)\n",
    "    h_relu = torch.relu(h)\n",
    "    y_pred = torch.relu(h).mm(W2)\n",
    "\n",
    "    # 計算loss\n",
    "    ###<your code>### \n",
    "    loss = ((y_pred - y)**2).sum()    #𝐿2_𝑙𝑜𝑠𝑠=(𝑦_𝑝𝑟𝑒𝑑 − 𝑦) **2 \n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "    ###<your code>###\n",
    "    y_pred_grad = 2. * (y_pred - y)\n",
    "    W2_grad = h_relu.T.mm(y_pred_grad)\n",
    "    \n",
    "    h_grad = y_pred_grad.mm(W2.T) * (h > 0.)\n",
    "    W1_grad = x.T.mm(h_grad)\n",
    "    \n",
    "    # 參數更新\n",
    "    ###<your code>###\n",
    "    W1.data -= learning_rate * W1_grad\n",
    "    W2.data -= learning_rate * W2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9XiShaU16s3"
   },
   "source": [
    "### 使用Pytorch的Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_VP1YW7516s4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "dlj3NwsP16s6",
    "outputId": "0463fd34-3edf-4516-9d36-c1143463790d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27579528.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6529b243a445>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# 倒傳遞: 計算W1與W2對loss的微分(梯度)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m###<your code>###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# 參數更新: 這裡再更新參數時，我們不希望更新參數的計算也被紀錄微分相關的資訊，因此使用torch.no_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# N: batch size\n",
    "# D_in: input dimension\n",
    "# H: hidden dimension\n",
    "# D_out: output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 隨機生成x, y\n",
    "###<your code>###\n",
    "x = torch.randn((N, D_in)).to(device)\n",
    "y = torch.randn((N, D_out)).to(device)\n",
    "\n",
    "# 初始化weight W1, W2\n",
    "###<your code>###\n",
    "W1 = torch.randn((D_in, H),requires_grad=True).to(device)\n",
    "W2 = torch.randn((H, D_out)requires_grad=True).to(device)\n",
    "\n",
    "# 設置learning rate\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 訓練500個epoch\n",
    "for t in range(500):\n",
    "    # 向前傳遞: 計算y_pred\n",
    "    ###<your code>###\n",
    "    y_pred = torch.mm(torch.relu(torch.mm(x, W1)), W2)   # 𝑦_𝑝𝑟𝑒𝑑 =𝑅𝑒𝐿𝑈(𝑋𝑊1)𝑊2\n",
    "    \n",
    "    # 計算loss\n",
    "    ###<your code>###\n",
    "    loss = ((y_pred - y)**2).sum()    #𝐿2_𝑙𝑜𝑠𝑠=(𝑦_𝑝𝑟𝑒𝑑 − 𝑦) **2 \n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 倒傳遞: 計算W1與W2對loss的微分(梯度)\n",
    "    ###<your code>###\n",
    "    loss.backward()\n",
    "    \n",
    "    # 參數更新: 這裡再更新參數時，我們不希望更新參數的計算也被紀錄微分相關的資訊，因此使用torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # 更新參數W1 W2\n",
    "        ###<your code>###\n",
    "        W1_grad -= learning_rate * W1.grad\n",
    "        W2_grad -= learning_rate * W2.grad\n",
    "        \n",
    "        # 將紀錄的gradient清空(因為已經更新參數)\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znJFnEdr16s9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "微分與倒傳遞作業.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
