{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128,64) #\"自行定義，只要確定模型能順利運行即可，參數值沒有一定限制\"\n",
    "        self.layer3 = LinearBNAC(64,32)   #\"自行定義，只要確定模型能順利運行即可，參數值沒有一定限制\"\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備輸入資料、優化器、標籤資料、模型輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=256,output_classes=10)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3) #\"使用Adam optimizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_features = 256\n",
    "dummy_input = torch.randn(batch_size, input_features,)\n",
    "\n",
    "#target = torch.empty(4, dtype=torch.float).random_(10)\n",
    "target = torch.tensor([9., 5., 4., 4.], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1281, 0.0298, 0.1143, 0.0676, 0.1014, 0.0749, 0.1000, 0.1726, 0.1084,\n",
      "         0.1028],\n",
      "        [0.0918, 0.0786, 0.0843, 0.0825, 0.2176, 0.1039, 0.0522, 0.1460, 0.0463,\n",
      "         0.0967],\n",
      "        [0.0985, 0.0626, 0.0813, 0.0393, 0.1698, 0.0804, 0.0773, 0.1298, 0.1414,\n",
      "         0.1196],\n",
      "        [0.0827, 0.0855, 0.1209, 0.0867, 0.1212, 0.0692, 0.0545, 0.1845, 0.1464,\n",
      "         0.0483]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input) #\"自行輸入\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算 CrossEntropy Loss\n",
    "* 請注意哪一個 Loss最適合：我們已經使用 softmax\n",
    "* 因為我們有使用dropout，並隨機產生dummy_input，所以各為學員得到的值會與解答不同，然而步驟原理需要相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NLLLoss() #\"自行輸入\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1055, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(torch.log(output), target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成back propagation並更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"自行輸入\"\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0328,  0.0061, -0.0109,  ..., -0.0024,  0.0199, -0.0413],\n",
      "        [-0.0507,  0.0474,  0.0406,  ...,  0.0329,  0.0588, -0.0184],\n",
      "        [-0.0202, -0.0311, -0.0160,  ...,  0.0602,  0.0303, -0.0624],\n",
      "        ...,\n",
      "        [ 0.0170,  0.0282,  0.0563,  ..., -0.0240,  0.0032,  0.0020],\n",
      "        [ 0.0507, -0.0617,  0.0491,  ...,  0.0297, -0.0535,  0.0485],\n",
      "        [-0.0525,  0.0109,  0.0463,  ...,  0.0157, -0.0084, -0.0477]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0077,  0.0035, -0.0011,  ...,  0.0050, -0.0044,  0.0030],\n",
      "        [ 0.0032,  0.0204, -0.0332,  ...,  0.0046,  0.0017, -0.0173],\n",
      "        [ 0.0032,  0.0084, -0.0048,  ...,  0.0200, -0.0048,  0.0100],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0028,  0.0030,  ..., -0.0028,  0.0044, -0.0007],\n",
      "        [ 0.0109, -0.0060,  0.0033,  ..., -0.0076,  0.0130, -0.0049],\n",
      "        [-0.0357, -0.0837, -0.0157,  ..., -0.2921, -0.2243, -0.1376]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"自行輸入\"\n",
    "optimizer.step()  # 使用 optimizer.step() 更新參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0318,  0.0051, -0.0099,  ..., -0.0034,  0.0209, -0.0423],\n",
      "        [-0.0517,  0.0464,  0.0416,  ...,  0.0319,  0.0578, -0.0174],\n",
      "        [-0.0212, -0.0321, -0.0150,  ...,  0.0592,  0.0313, -0.0634],\n",
      "        ...,\n",
      "        [ 0.0160,  0.0292,  0.0553,  ..., -0.0230,  0.0022,  0.0030],\n",
      "        [ 0.0497, -0.0607,  0.0481,  ...,  0.0307, -0.0545,  0.0495],\n",
      "        [-0.0515,  0.0119,  0.0473,  ...,  0.0167, -0.0074, -0.0467]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0078,  0.0035, -0.0011,  ...,  0.0050, -0.0044,  0.0029],\n",
      "        [ 0.0031,  0.0204, -0.0331,  ...,  0.0046,  0.0017, -0.0173],\n",
      "        [ 0.0032,  0.0084, -0.0048,  ...,  0.0201, -0.0048,  0.0099],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0028,  0.0031,  ..., -0.0029,  0.0044, -0.0007],\n",
      "        [ 0.0110, -0.0060,  0.0034,  ..., -0.0076,  0.0130, -0.0048],\n",
      "        [-0.0358, -0.0837, -0.0156,  ..., -0.2921, -0.2243, -0.1377]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清空 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"自行輸入\"\n",
    "optimizer.zero_grad() # 清理目前參數上所殘留的梯度，並開始下一個 iteration 的計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0318,  0.0051, -0.0099,  ..., -0.0034,  0.0209, -0.0423],\n",
      "        [-0.0517,  0.0464,  0.0416,  ...,  0.0319,  0.0578, -0.0174],\n",
      "        [-0.0212, -0.0321, -0.0150,  ...,  0.0592,  0.0313, -0.0634],\n",
      "        ...,\n",
      "        [ 0.0160,  0.0292,  0.0553,  ..., -0.0230,  0.0022,  0.0030],\n",
      "        [ 0.0497, -0.0607,  0.0481,  ...,  0.0307, -0.0545,  0.0495],\n",
      "        [-0.0515,  0.0119,  0.0473,  ...,  0.0167, -0.0074, -0.0467]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
